{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb70f9a-c1db-46dd-b3e1-b452d5ef5441",
   "metadata": {},
   "source": [
    "- Simplified self-attention, without trainable weights\n",
    "- Self-attention witn trainable weights\n",
    "- Causal attention\n",
    "- Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873d0a17-48c3-420d-8493-55bb0faa9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9442e2-ddf8-458d-b3d1-b3da111cf5da",
   "metadata": {},
   "source": [
    "**a) Simplified self-attention (without trainabel weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b9a9b54-a689-4579-b937-ee3ea5fd3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example. input sentence embedded into 3-dimensional vectors.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fef82-eb68-4e4c-bf14-1916ef6dc265",
   "metadata": {},
   "source": [
    "Step 1. Calculate attention scores. w21 is the attention xscore between x^1 and the emedded query token q2, \n",
    "#which in this cases is x^2\n",
    "- q2=x2\n",
    "- w21=x1q2\n",
    "- w22=x2q2\n",
    "- w23=x3q2\n",
    "- w24=x4q2\n",
    "- w25=x5q2\n",
    "- w26=x6q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c735b0e1-56a0-4e71-a347-a83ce2e30bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "attn_scores_2=torch.empty(inputs.shape[0]) #to store attention scores for x2\n",
    "for i,xi in enumerate(inputs):\n",
    "    attn_scores_2[i]=torch.dot(xi,query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8e542-1c64-43d1-8724-20f6782d0a42",
   "metadata": {},
   "source": [
    "Step 2. Normalize the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c919035c-109c-45b5-a32a-4f583e4136f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "#Normalizing using softmax (better at handling extreme values and has more desirable gradient properties during training)\n",
    "attn_weights_2= torch.softmax(attn_weights_2,dim=0)\n",
    "print(attn_weights_2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cf2ff-37c9-4165-bdb3-71e79bd32bd8",
   "metadata": {},
   "source": [
    "Step 3. Compute the context vector z2 by multiplying the embedded input tokens xi with the attention weights and sum the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4f35076-6b50-4643-8864-6ee88da83a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "context_vec_2=torch.zeros(query.shape) #to store attention scores for x2\n",
    "for i,xi in enumerate(inputs):\n",
    "    context_vec_2+=attn_weights_2[i]*xi\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c566dc3a-c81a-4b1e-bcdc-50431f9b244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Computing for all input tokens\n",
    "attn_scores=torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "for i, xi in enumerate(inputs):\n",
    "    for j,xj in enumerate(inputs):\n",
    "            attn_scores[i,j]=torch.dot(xi,xj)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8282d052-0348-4936-af29-21581ced0989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Faster way \n",
    "attn_scores=inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6e2773a-58da-4cd8-b4c4-025823905a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#Normalizing\n",
    "attn_weights=torch.softmax(attn_scores,dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af1e2a0a-ca2b-4787-99e1-4bfe7d45ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "#Context vector\n",
    "all_context_vecs=attn_weights@inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dcb4-db8b-4f7d-9108-e577f94d5271",
   "metadata": {},
   "source": [
    "**b) Self-attention with trainable weights (scaled dot-product attention)**\n",
    "\n",
    "Difference with the simplified model before is the introduction of trainable weight matrices that are updated during model training. There are three weigh matrices Wq, Wk, and Wv used to project the embedded input tokens xi into query, key, and value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb66dabf-850a-4575-b753-e60f8a3716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2 #the input and output dimensions are usually the same, but for illustration purposes and easier computation selected 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b227052-9603-48ca-8161-d7b6ab888b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d959090-1b96-4136-9ff5-84cf749c67e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19feba6f-fcd5-4182-8f46-28ad0123f67a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d005c6-d55b-4d52-be14-d4e794f44ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c15649-8e22-4374-a90c-11b6fd336da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959e3edb-bc73-4af6-a494-6d2218d3ac0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
