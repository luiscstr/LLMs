{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bb70f9a-c1db-46dd-b3e1-b452d5ef5441",
   "metadata": {},
   "source": [
    "- Simplified self-attention, without trainable weights\n",
    "- Self-attention witn trainable weights\n",
    "- Causal attention\n",
    "- Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "873d0a17-48c3-420d-8493-55bb0faa9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9442e2-ddf8-458d-b3d1-b3da111cf5da",
   "metadata": {},
   "source": [
    "**a) Simplified self-attention (without trainable weights)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b9a9b54-a689-4579-b937-ee3ea5fd3c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example. input sentence embedded into 3-dimensional vectors.\n",
    "inputs = torch.tensor(\n",
    "[[0.43, 0.15, 0.89], # Your (x^1)\n",
    "[0.55, 0.87, 0.66], # journey (x^2)\n",
    "[0.57, 0.85, 0.64], # starts (x^3)\n",
    "[0.22, 0.58, 0.33], # with (x^4)\n",
    "[0.77, 0.25, 0.10], # one (x^5)\n",
    "[0.05, 0.80, 0.55]] # step (x^6)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492fef82-eb68-4e4c-bf14-1916ef6dc265",
   "metadata": {},
   "source": [
    "Step 1. Calculate attention scores. w21 is the attention xscore between x^1 and the emedded query token q2, \n",
    "#which in this cases is x^2\n",
    "- q2=x2\n",
    "- w21=x1q2\n",
    "- w22=x2q2\n",
    "- w23=x3q2\n",
    "- w24=x4q2\n",
    "- w25=x5q2\n",
    "- w26=x6q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c735b0e1-56a0-4e71-a347-a83ce2e30bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "attn_scores_2=torch.empty(inputs.shape[0]) #to store attention scores for x2\n",
    "for i,xi in enumerate(inputs):\n",
    "    attn_scores_2[i]=torch.dot(xi,query)\n",
    "print(attn_scores_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a8e542-1c64-43d1-8724-20f6782d0a42",
   "metadata": {},
   "source": [
    "Step 2. Normalize the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c919035c-109c-45b5-a32a-4f583e4136f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n"
     ]
    }
   ],
   "source": [
    "#Normalizing using softmax (better at handling extreme values and has more desirable gradient properties during training)\n",
    "attn_weights_2= torch.softmax(attn_weights_2,dim=0)\n",
    "print(attn_weights_2_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69cf2ff-37c9-4165-bdb3-71e79bd32bd8",
   "metadata": {},
   "source": [
    "Step 3. Compute the context vector z2 by multiplying the embedded input tokens xi with the attention weights and sum the resulting vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c4f35076-6b50-4643-8864-6ee88da83a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4419, 0.6515, 0.5683])\n"
     ]
    }
   ],
   "source": [
    "query=inputs[1]\n",
    "context_vec_2=torch.zeros(query.shape) #to store attention scores for x2\n",
    "for i,xi in enumerate(inputs):\n",
    "    context_vec_2+=attn_weights_2[i]*xi\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c566dc3a-c81a-4b1e-bcdc-50431f9b244b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Computing for all input tokens\n",
    "attn_scores=torch.empty(inputs.shape[0],inputs.shape[0])\n",
    "for i, xi in enumerate(inputs):\n",
    "    for j,xj in enumerate(inputs):\n",
    "            attn_scores[i,j]=torch.dot(xi,xj)\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8282d052-0348-4936-af29-21581ced0989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
      "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
      "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
      "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
      "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
      "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
     ]
    }
   ],
   "source": [
    "#Faster way \n",
    "attn_scores=inputs@inputs.T\n",
    "print(attn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a6e2773a-58da-4cd8-b4c4-025823905a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
      "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
      "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
      "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
      "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
      "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
     ]
    }
   ],
   "source": [
    "#Normalizing\n",
    "attn_weights=torch.softmax(attn_scores,dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "af1e2a0a-ca2b-4787-99e1-4bfe7d45ec98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4421, 0.5931, 0.5790],\n",
      "        [0.4419, 0.6515, 0.5683],\n",
      "        [0.4431, 0.6496, 0.5671],\n",
      "        [0.4304, 0.6298, 0.5510],\n",
      "        [0.4671, 0.5910, 0.5266],\n",
      "        [0.4177, 0.6503, 0.5645]])\n"
     ]
    }
   ],
   "source": [
    "#Context vector\n",
    "all_context_vecs=attn_weights@inputs\n",
    "print(all_context_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b779dcb4-db8b-4f7d-9108-e577f94d5271",
   "metadata": {},
   "source": [
    "**b) Self-attention with trainable weights (scaled dot-product attention)**\n",
    "\n",
    "Difference with the simplified model before is the introduction of trainable weight matrices that are updated during model training. There are three weigh matrices Wq, Wk, and Wv used to project the embedded input tokens xi into query, key, and value vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "eb66dabf-850a-4575-b753-e60f8a3716b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=inputs[1]\n",
    "d_in=inputs.shape[1]\n",
    "d_out=2 #the input and output dimensions are usually the same, but for illustration purposes and easier computation selected 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1b227052-9603-48ca-8161-d7b6ab888b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization of the three weight matrices Wq, Wk and and Wv. requires_grad set to False to simplify the outputs,\n",
    "#but should be set to True for model training\n",
    "torch.manual_seed(123)\n",
    "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
    "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d959090-1b96-4136-9ff5-84cf749c67e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4306, 1.4551])\n"
     ]
    }
   ],
   "source": [
    "query_2=x2@W_query\n",
    "key_2=x2@W_key\n",
    "value_2=x2@W_value\n",
    "print(query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "19feba6f-fcd5-4182-8f46-28ad0123f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys.shape: torch.Size([6, 2])\n",
      "values.shape: torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "#Obtaining key and value vectors for all input elements\n",
    "keys = inputs @ W_key\n",
    "values = inputs @ W_value\n",
    "print(\"keys.shape:\", keys.shape)\n",
    "print(\"values.shape:\", values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "68d005c6-d55b-4d52-be14-d4e794f44ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.8524)\n"
     ]
    }
   ],
   "source": [
    "#Compute attention scores. Starting for w22.\n",
    "keys_2 = keys[1] #A\n",
    "attn_score_22 = query_2.dot(keys_2)\n",
    "print(attn_score_22)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "69c15649-8e22-4374-a90c-11b6fd336da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
     ]
    }
   ],
   "source": [
    "#All attention scores\n",
    "attn_scores_2 = query_2 @ keys.T # All attention scores for given query\n",
    "print(attn_scores_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e017841c-c7ce-44ce-99c8-f0f5a1833eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
     ]
    }
   ],
   "source": [
    "#Compute attention weights by scaling. The difference is that the attention scores are divided by the square root\n",
    "#of the embedding dimension of the keys. This is to improve the training performance by avoiding small gradients.\n",
    "d_k = keys.shape[-1]\n",
    "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
    "print(attn_weights_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2df2cc48-e856-4fa3-8ecc-88d0a3f83167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3061, 0.8210])\n"
     ]
    }
   ],
   "source": [
    "#Computing context vector\n",
    "context_vec_2 = attn_weights_2 @ values\n",
    "print(context_vec_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "53124a6a-cffc-4e32-91d7-21c82550474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1855, 0.8812],\n",
       "        [0.3951, 1.0037],\n",
       "        [0.3879, 0.9831],\n",
       "        [0.2393, 0.5493],\n",
       "        [0.1492, 0.3346],\n",
       "        [0.3221, 0.7863]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "959e3edb-bc73-4af6-a494-6d2218d3ac0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c996a0d6-455e-460e-b634-b97bc6182f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implementing a self attention class\n",
    "class SelfAttention_v1(nn.Module):\n",
    "    def __init__(self,d_in,d_out):\n",
    "        super().__init__()\n",
    "        self.d_out=d_out\n",
    "        self.W_query=nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_key=nn.Parameter(torch.rand(d_in, d_out))\n",
    "        self.W_value=nn.Parameter(torch.rand(d_in, d_out))\n",
    "\n",
    "    def forward(self,x):\n",
    "        keys=x@self.W_key\n",
    "        queries=x@self.W_query\n",
    "        values=x@self.W_value\n",
    "        attn_scores=queries@keys.T\n",
    "        attn_weights=torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec=attn_weights@values\n",
    "        return context_vec\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61240923-299f-4ae8-95c0-f7e46cc49579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2996, 0.8053],\n",
      "        [0.3061, 0.8210],\n",
      "        [0.3058, 0.8203],\n",
      "        [0.2948, 0.7939],\n",
      "        [0.2927, 0.7891],\n",
      "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
    "print(sa_v1(inputs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "23038d05-dde5-419c-bcde-dfc11e1125b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improved by repalcing random by linear layers,  which have an optimized weight initialization scheme \n",
    "class SelfAttention_v2(nn.Module):\n",
    "    def __init__(self,d_in,d_out,qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out=d_out\n",
    "        self.W_query=nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key=nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value=nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "    def forward(self,x):\n",
    "        keys=self.W_key(x)\n",
    "        queries=self.W_query(x)\n",
    "        values=self.W_value(x)\n",
    "        attn_scores=queries@keys.T\n",
    "        attn_weights=torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        context_vec=attn_weights@values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "15080bc5-069c-44ae-b729-f102abaea925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0739,  0.0713],\n",
      "        [-0.0748,  0.0703],\n",
      "        [-0.0749,  0.0702],\n",
      "        [-0.0760,  0.0685],\n",
      "        [-0.0763,  0.0679],\n",
      "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(789)#Missing in book\n",
    "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
    "print(sa_v2(inputs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916a8396-8fa3-46a7-bd3f-179c298f1dba",
   "metadata": {},
   "source": [
    "**Casual/masked attention**\n",
    "\n",
    "Restricts a model to only consider previous and current inputs in a sequence when processing any given token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7b9218eb-2863-4c0e-ba1a-199ffc8b45ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
      "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
      "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Calculating the attention weights reusing the previous calculated weights\n",
    "queries = sa_v2.W_query(inputs) \n",
    "keys = sa_v2.W_key(inputs)\n",
    "attn_scores = queries @ keys.T\n",
    "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cdbdf7f8-7297-4b79-a4e3-5fd504fde99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0., 0.],\n",
      "        [1., 1., 0., 0., 0., 0.],\n",
      "        [1., 1., 1., 0., 0., 0.],\n",
      "        [1., 1., 1., 1., 0., 0.],\n",
      "        [1., 1., 1., 1., 1., 0.],\n",
      "        [1., 1., 1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "#Masking using the trill function\n",
    "block_size = attn_scores.shape[0]\n",
    "mask_simple = torch.tril(torch.ones(block_size, block_size))\n",
    "print(mask_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b4911afe-376e-432f-b850-c026f266468a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
      "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
      "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "masked_simple=attn_weights*mask_simple\n",
    "print(masked_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "abf99a9a-ee49-452e-8004-5c0533cec117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Renormalizing\n",
    "row_sums = masked_simple.sum(dim=1, keepdim=True)\n",
    "masked_simple_norm = masked_simple / row_sums\n",
    "print(masked_simple_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d81c4cce-edc4-4ad0-81d5-9bedf98ec9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
      "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
      "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
      "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
      "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#More efficient way computing the masked attention weights with the softmax function. The softmax function converts\n",
    "#its inputs into a probability distribution. When negative infinity values a (-âˆž) are present in a row, the softmax \n",
    "#function treats them as zero probability\n",
    "mask = torch.triu(torch.ones(block_size, block_size), diagonal=1)\n",
    "masked = attn_scores.masked_fill(mask.bool(), -torch.inf) #Tensor.masked_fill_(mask, value) Fills elements of self tensor with value where mask is True.\n",
    "print(masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3d8614b1-f8c7-4ff6-a8ca-2eb8d7529a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
      "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
      "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
      "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=1)\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ccc126fc-c87e-4c97-b909-d292d515d7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2., 2., 2., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.],\n",
      "        [0., 0., 2., 0., 2., 0.],\n",
      "        [2., 2., 0., 0., 0., 2.],\n",
      "        [2., 0., 0., 0., 0., 2.],\n",
      "        [0., 2., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#Masking additional attention weights with dropout (to prevent overfitting)In the transformer architecture, dropout in the attention\n",
    "#mechanism is typically applied in two specific areas: after calculating the attention scoresor after applying the attention weights \n",
    "#to the value vectors\n",
    "torch.manual_seed(123)\n",
    "dropout = torch.nn.Dropout(0.5) #dropout rate of 0.5\n",
    "example = torch.ones(6, 6) \n",
    "print(dropout(example))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b139abf6-d34a-48a8-91c5-cbb381e92beb",
   "metadata": {},
   "source": [
    "Note:  To compensate for the reduction in active elements, the values of the remaining elements in the matrix are scaled up by a\n",
    "factor of 1/dropout rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1d98e6a9-7229-4ac2-89bf-743c094bbe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
      "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "print(dropout(attn_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "73f0bf7d-b6d1-4e3d-a1e2-5e1a3c65e823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n"
     ]
    }
   ],
   "source": [
    "#Adding batch definition to handle batches consisting of more than one input. In this example we use 2 input texts with 6 tokens each, \n",
    "#where each token is a 3-dimensional embedding vector\n",
    "batch = torch.stack((inputs, inputs), dim=0)# \n",
    "print(batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "21925f12-d143-48c1-8ed7-32376e5d038a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Class\n",
    "class CausalAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, block_size, dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) #A\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(block_size, block_size), diagonal=1)) \n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # transpose dimensions 1 and 2, keeping the batch dimension at the first position (0)\n",
    "        attn_scores.masked_fill_(self.mask.bool()[:num_tokens, :num_tokens], -torch.inf) \n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f9ab610f-e4e2-40b0-91d9-39c286154742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]],\n",
      "\n",
      "        [[-0.4519,  0.2216],\n",
      "         [-0.5874,  0.0058],\n",
      "         [-0.6300, -0.0632],\n",
      "         [-0.5675, -0.0843],\n",
      "         [-0.5526, -0.0981],\n",
      "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "block_size = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, block_size, 0.0)\n",
    "print(context_vecs)\n",
    "context_vecs = ca(batch)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b55cb3-7130-4dbd-a87c-afbb87e7a191",
   "metadata": {},
   "source": [
    "**Multi-head attention**\n",
    "\n",
    "Extending the previous casual attention over multiple-heads. Instead a single matrices Wv, Wq and Wk there are x weight matrices Wvx, Wqx and Wkx to obtain x context vecots Zx. THe idea is to run the attention mechanism milte times in prallel with different learned linear projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "813d051a-ce24-4bb2-80d0-3898f3879a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
      "\n",
      "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
      "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
      "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
      "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
      "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
      "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 4])\n"
     ]
    }
   ],
   "source": [
    "#Creating  multi head attention wrapper\n",
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) \n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "000f184c-e6d0-44e9-a71a-9edba67efb10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
      "context_vecs.shape: torch.Size([2, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "#More efficient way instead of using two separate clases (MultiHeadAttentionWrapper and CausalAttention). Single W1, Wk, and Wv weight matrices\n",
    "#are created and split into individual matrices for each attention head.\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec) # optional projection\n",
    "\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "batch_size, context_length, d_in = batch.shape\n",
    "d_out = 2\n",
    "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613518b2-ca7f-46a6-8b57-8f62647231ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780d4b9-faa3-44b4-a744-e0c80693a167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a0b42a-29bc-4049-9e44-de6ee6f3d81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
