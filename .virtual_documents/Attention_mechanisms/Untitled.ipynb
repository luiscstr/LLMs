


import torch
import torch.nn as nn






#Example. input sentence embedded into 3-dimensional vectors.
inputs = torch.tensor(
[[0.43, 0.15, 0.89], # Your (x^1)
[0.55, 0.87, 0.66], # journey (x^2)
[0.57, 0.85, 0.64], # starts (x^3)
[0.22, 0.58, 0.33], # with (x^4)
[0.77, 0.25, 0.10], # one (x^5)
[0.05, 0.80, 0.55]] # step (x^6)
)






query=inputs[1]
attn_scores_2=torch.empty(inputs.shape[0]) #to store attention scores for x2
for i,xi in enumerate(inputs):
    attn_scores_2[i]=torch.dot(xi,query)
print(attn_scores_2)





#Normalizing using softmax (better at handling extreme values and has more desirable gradient properties during training)
attn_weights_2= torch.softmax(attn_weights_2,dim=0)
print(attn_weights_2_norm)





query=inputs[1]
context_vec_2=torch.zeros(query.shape) #to store attention scores for x2
for i,xi in enumerate(inputs):
    context_vec_2+=attn_weights_2[i]*xi
print(context_vec_2)


#Computing for all input tokens
attn_scores=torch.empty(inputs.shape[0],inputs.shape[0])
for i, xi in enumerate(inputs):
    for j,xj in enumerate(inputs):
            attn_scores[i,j]=torch.dot(xi,xj)
print(attn_scores)


#Faster way 
attn_scores=inputs@inputs.T
print(attn_scores)


#Normalizing
attn_weights=torch.softmax(attn_scores,dim=1)
print(attn_weights)


#Context vector
all_context_vecs=attn_weights@inputs
print(all_context_vecs)





x2=inputs[1]
d_in=inputs.shape[1]
d_out=2 #the input and output dimensions are usually the same, but for illustration purposes and easier computation selected 2


#Initialization of the three weight matrices Wq, Wk and and Wv. requires_grad set to False to simplify the outputs,
#but should be set to True for model training
torch.manual_seed(123)
W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)
W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)


query_2=x2@W_query
key_2=x2@W_key
value_2=x2@W_value
print(query_2)


#Obtaining key and value vectors for all input elements
keys = inputs @ W_key
values = inputs @ W_value
print("keys.shape:", keys.shape)
print("values.shape:", values.shape)


#Compute attention scores. Starting for w22.
keys_2 = keys[1] #A
attn_score_22 = query_2.dot(keys_2)
print(attn_score_22)




#All attention scores
attn_scores_2 = query_2 @ keys.T # All attention scores for given query
print(attn_scores_2)



#Compute attention weights by scaling. The difference is that the attention scores are divided by the square root
#of the embedding dimension of the keys. This is to improve the training performance by avoiding small gradients.
d_k = keys.shape[-1]
attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)
print(attn_weights_2)



#Computing context vector
context_vec_2 = attn_weights_2 @ values
print(context_vec_2)


values


attn_weights_2


#Implementing a self attention class
class SelfAttention_v1(nn.Module):
    def __init__(self,d_in,d_out):
        super().__init__()
        self.d_out=d_out
        self.W_query=nn.Parameter(torch.rand(d_in, d_out))
        self.W_key=nn.Parameter(torch.rand(d_in, d_out))
        self.W_value=nn.Parameter(torch.rand(d_in, d_out))

    def forward(self,x):
        keys=x@self.W_key
        queries=x@self.W_query
        values=x@self.W_value
        attn_scores=queries@keys.T
        attn_weights=torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec=attn_weights@values
        return context_vec

    


torch.manual_seed(123)
sa_v1 = SelfAttention_v1(d_in, d_out)
print(sa_v1(inputs))



#Improved by repalcing random by linear layers,  which have an optimized weight initialization scheme 
class SelfAttention_v2(nn.Module):
    def __init__(self,d_in,d_out,qkv_bias=False):
        super().__init__()
        self.d_out=d_out
        self.W_query=nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_key=nn.Linear(d_in, d_out, bias=qkv_bias)
        self.W_value=nn.Linear(d_in, d_out, bias=qkv_bias)

    def forward(self,x):
        keys=self.W_key(x)
        queries=self.W_query(x)
        values=self.W_value(x)
        attn_scores=queries@keys.T
        attn_weights=torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)
        context_vec=attn_weights@values
        return context_vec


torch.manual_seed(123)
sa_v2 = SelfAttention_v2(d_in, d_out)
print(sa_v2(inputs))










